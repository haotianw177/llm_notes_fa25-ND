<!-- #! ariamis -->

# notes 

## short questions

In language modeling, the pretraining phase is self-supervised, meaning that labels are obtained automatically from raw text, such as masking a word for prediction.

GPT-1 pre-trained on Next word prediction

BART, T5, GPT could be used for this task setup of: 1. Input: STEM lecture recording 2. Output: A multiple-choice quiz question

> about transformers:
> Without a positional encoding layer for transformer-based architectures, the architecture cannot make sense of the order of tokens
>> Transformers are the backbone behind models like BERT, GPT, and T5
>>> Bidirectional transformer-based architectures like BERT are good at context understanding, while unidirectional transformer-based architectures like GPT can generate text well.





## long questions

What are the scaling laws for language models? Estimate the data size and compute required to train GPT-3.

What are the key differences between In-Context Learning (ICL) and Supervised Learning?
