<!-- #! ariamis -->

# notes 

## short questions

In language modeling, the pretraining phase is self-supervised, meaning that labels are obtained automatically from raw text, such as masking a word for prediction.

GPT-1 pre-trained on Next word prediction


BART, T5, GPT could be used for this task setup of:
1. Input: STEM lecture recording
2. Output: A multiple-choice quiz question






## long questions